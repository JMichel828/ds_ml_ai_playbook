{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Evaluation Metrics\n",
        "\n",
        "This notebook is a **companion to `02_evaluation_metrics.md`**.\n",
        "\n",
        "Purpose:\n",
        "- Build intuition for common evaluation metrics\n",
        "- Understand tradeoffs between metrics\n",
        "- Practice interview-style reasoning\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix, roc_curve\n",
        "\n",
        "np.random.seed(42)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Simulated Imbalanced Classification Problem\n",
        "\n",
        "We intentionally create imbalance to show why metric choice matters.\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "n = 1000\n",
        "y_true = np.random.choice([0, 1], size=n, p=[0.9, 0.1])\n",
        "\n",
        "y_scores = y_true * np.random.uniform(0.4, 1.0, size=n) + (1 - y_true) * np.random.uniform(0.0, 0.6, size=n)\n",
        "y_pred = (y_scores >= 0.5).astype(int)\n",
        "\n",
        "pd.Series(y_true).value_counts(normalize=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Confusion Matrix\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "cm = confusion_matrix(y_true, y_pred)\n",
        "pd.DataFrame(cm, index=['Actual 0', 'Actual 1'], columns=['Pred 0', 'Pred 1'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Core Metrics\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "pd.DataFrame({\n",
        "    'Accuracy': [accuracy_score(y_true, y_pred)],\n",
        "    'Precision': [precision_score(y_true, y_pred)],\n",
        "    'Recall': [recall_score(y_true, y_pred)],\n",
        "    'F1': [f1_score(y_true, y_pred)],\n",
        "    'ROC AUC': [roc_auc_score(y_true, y_scores)]\n",
        "}).T.rename(columns={0: 'Value'})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ROC Curve\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "fpr, tpr, _ = roc_curve(y_true, y_scores)\n",
        "\n",
        "plt.figure(figsize=(6, 4))\n",
        "plt.plot(fpr, tpr, label='ROC Curve')\n",
        "plt.plot([0, 1], [0, 1], '--', label='Random')\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}

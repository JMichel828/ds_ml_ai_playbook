# Common Probability Distributions

## Core Concept & Intuition

A probability distribution describes **how outcomes are generated and spread**, not just their average.
For managers and senior managers, understanding distributions is critical because **every metric implicitly assumes one**.

Wrong distributional assumptions lead to:
- overconfidence in results
- underestimating risk
- poor experiment design
- fragile models in production

This document focuses on **intuition + simulation**, not formulas.

---

## Why Distribution Choice Matters

Different real-world processes produce fundamentally different shapes:

- Binary outcomes → Bernoulli
- Counts of events → Binomial or Poisson
- Aggregated noise → Normal
- Revenue, latency, spend → Lognormal (heavy-tailed)

Knowing *why* matters more than memorizing equations.

---

## Bernoulli Distribution (Binary Outcomes)

### Intuition
A single trial with two outcomes:
- click / no click
- convert / not convert
- fraud / not fraud

Values: {0, 1}

---

### Simulation: Conversion Event

```python
import numpy as np

np.random.seed(0)

n = 100_000
conversion = np.random.binomial(1, 0.08, n)

np.mean(conversion), np.var(conversion)
```

**Key insights:**
- Mean = probability of success
- Variance = p(1−p)
- Rare events have high relative uncertainty

---

## Binomial Distribution (Repeated Bernoulli Trials)

### Intuition
Counts of successes across many independent trials:
- number of conversions in an experiment
- number of defects in a batch

---

### Simulation: Experiment Outcomes

```python
trials = 1_000
successes = np.random.binomial(trials, 0.08, size=50_000)

np.mean(successes), np.var(successes)
```

**Manager takeaway:**
- Most A/B test statistics rely on binomial assumptions
- Independence is often violated (users influence each other)

---

## Poisson Distribution (Event Counts Over Time)

### Intuition
Models counts of events that occur:
- independently
- at a constant average rate
- within a fixed window

Examples:
- fraud attempts per day
- tickets per hour

---

### Simulation: Ideal Poisson Process

```python
events = np.random.poisson(lam=4, size=100_000)

np.mean(events), np.var(events)
```

Key property:
> Mean ≈ Variance

---

### When Poisson Fails (Overdispersion)

```python
rates = np.random.gamma(shape=2, scale=2, size=100_000)
events = np.random.poisson(lam=rates)

np.mean(events), np.var(events)
```

**Insight:**
- Variance >> mean signals heterogeneity
- Common in real user behavior

---

## Normal Distribution (Aggregated Effects)

### Intuition
Appears when many small, independent effects add together.

Common uses:
- metric noise
- regression errors
- CLT approximations

---

### Simulation: Measurement Noise

```python
noise = np.random.normal(0, 1, 100_000)

np.mean(noise), np.var(noise)
```

---

### Practical Warning

Normality is often **assumed**, not verified.

Bad assumptions for:
- revenue
- latency
- session duration

---

## Lognormal Distribution (Heavy-Tailed Metrics)

### Intuition
Generated by **multiplicative processes**:
- user spend
- response times
- engagement

Right-skewed with long tails.

---

### Simulation: Revenue Per User

```python
revenue = np.random.lognormal(mean=1.0, sigma=1.4, size=100_000)

np.mean(revenue), np.median(revenue)
```

**Key insight:**
- Mean is dominated by a small minority
- Median is more stable
- Variance is extremely high

---

## Comparing Distributions Side by Side

```python
np.percentile(revenue, [50, 90, 99])
```

Tail behavior matters more than averages.

---

## Choosing the Right Distribution (Decision Guide)

| Metric Type | Likely Distribution |
|------------|--------------------|
| Click / convert | Bernoulli |
| # successes | Binomial |
| Events per time | Poisson |
| Aggregated noise | Normal |
| Revenue / latency | Lognormal |

---

## System-Level Considerations

At scale:
- Distribution shapes drift over time
- Aggregation hides tail risk
- Alerting systems miss rare events
- Mean-optimized systems fail catastrophically

Common failure:
> Teams optimize the mean while ignoring the tail.

---

## Common Pitfalls

- Assuming normality by default
- Ignoring overdispersion
- Using averages only
- Applying Poisson without validating variance
- Misinterpreting heavy-tailed data

---

## Interview Guidance

### How Interviewers Test This
- “What distribution fits this metric?”
- “Why is this metric so noisy?”
- “What assumptions are you making?”

### Strong Interview Framing

> “I reason about the data-generating process first,  
> then choose the distribution that matches it.”

---

## Summary Checklist

- [ ] I understand why different processes produce different distributions
- [ ] I can simulate distributions to sanity-check assumptions
- [ ] I know when normal assumptions break
- [ ] I can explain tail risk clearly
- [ ] I connect distribution choice to real decisions
